{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88ed806-8b8d-4190-947d-9889483a6102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1390174-576c-4858-a59d-4fc509df8e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1eb54ea-627f-419d-9230-d28540fc299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c2c2b3-e9d5-4b07-aff8-a4f82f96c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from this are in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "106945f9-0ceb-4227-ad41-cde2d2c652ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b965eb4b-3fbd-4ec9-aa9f-06c6eb24ff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "#  punkt module is a pre-trained model that helps you tokenize words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce54dd-2bac-4153-a6c2-579d9fa65d39",
   "metadata": {},
   "source": [
    "## 1. Tokenizacia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56567b1-120a-4bcd-a79d-ddb76044bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb0698-b437-4339-81ae-f508da2f93d1",
   "metadata": {},
   "source": [
    "## 2. Normalizacia\n",
    "- converting a word to its canonical form. \n",
    "- stemming and lemmatization are two popular techniques of normalization\n",
    "- stemming is a process of removing affixes from a word\n",
    "- lemmatization normalizes a word with the context of vocabulary and morphological analysis of words in text. The lemmatization algorithm analyzes the structure of the word and its context to convert it to a normalized form. Therefore, it comes at a cost of speed. A comparison of stemming and lemmatization ultimately comes down to a trade off between speed and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6fbe42-bf10-43ad-94f5-18236a25b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet is a lexical database for the English language that helps the script determine the base word\n",
    "# averaged_perceptron_tagger resource determines the context of a word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080cca55-ab73-4dee-ae74-998c2c8e0d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba903e0-cb15-441f-9c8a-64dcd99d1a59",
   "metadata": {},
   "source": [
    "### tagging algorithm: pos_tag function:\n",
    "- NNP: Noun, proper, singular\n",
    "- NN: Noun, common, singular or mass\n",
    "- IN: Preposition or conjunction, subordinating\n",
    "- VBG: Verb, gerund or present participle\n",
    "- VBN: Verb, past participle  \n",
    "\n",
    "full list: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html  \n",
    "In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa2b677-6c35-40c5-9c57-525d676123ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To incorporate this into a function that normalizes a sentence, you should first generate the tags for each token in the text, and then lemmatize each word using the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d1710e-b185-4dcf-9463-46aef23e75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a450a89-f7aa-497a-89ee-7f3de958c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c8c112-6790-42a7-89d4-e83933e11b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc31cdc-3578-4c4b-b074-c0eefc832767",
   "metadata": {},
   "source": [
    "## 3. Removing Noise from the Data  \n",
    "- Stop words\n",
    "- Punctuation and special characters\n",
    "- \\# and @ in tweets \n",
    "- hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a239f78b-6fc0-48d5-9682-2f4d24230161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "645d61d7-f880-409d-8c13-0f87be6c5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5198f6f0-ead5-4a35-bb31-a3079368d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b8641d-e919-494e-928f-cd37d236edc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(remove_noise(tweet_tokens[0], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05402ab4-e4ab-4fc5-aca0-d9ae09615577",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf87e5d-7981-447b-a59e-967dce26ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c197ca1-e238-44f5-917f-951e4a4fffbc",
   "metadata": {},
   "source": [
    "## 4. Determining Word Density  \n",
    "- word frequency\n",
    "- you can find out which are the most common words using the FreqDist class of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "154fc0bd-1ba9-4221-8b03-d6b0f3507b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c265364-c8af-4676-ad6c-ca31edbbcc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbacae3-a186-4758-998c-1c4b235e77c0",
   "metadata": {},
   "source": [
    "To summarize, you extracted the tweets from nltk, tokenized, normalized, and cleaned up the tweets for using in the model. Finally, you also looked at the frequencies of tokens in the data and checked the frequencies of the top ten tokens.  \n",
    "\n",
    "In the next step you will prepare data for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c695a-82a2-469a-a1aa-af3f630e7c3c",
   "metadata": {},
   "source": [
    "## 5. Preparing data for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806257cc-914e-4bb7-955a-8bf2c2529c97",
   "metadata": {},
   "source": [
    "- will use the Naive Bayes classifier in NLTK to perform the modeling\n",
    "- the model requires not just a list of words in a tweet, but a Python dictionary with words as keys and True as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c32d009-dfa7-40e7-9153-dd7f7ad57e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "603482a4-aae8-42b0-b7df-99d4b48635c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code attaches a Positive or Negative label to each tweet. It then creates a dataset by joining the positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "736d0f52-4288-47c6-ac6f-4d0acdf79922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945e05d-6008-46a7-b654-02c4b2267a2c",
   "metadata": {},
   "source": [
    "By default, the data contains all positive tweets followed by all negative tweets in sequence. When training the model, you should provide a sample of your data that does not contain any bias. To avoid bias, you’ve added code to randomly arrange the data using the .shuffle() method of random.  \n",
    "In this step, you converted the cleaned tokens to a dictionary form, randomly shuffled the dataset, and split it into training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b2c17-a3ea-4834-bd06-b1fdc20ca085",
   "metadata": {},
   "source": [
    "## 6. Building and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52583d62-f7ba-46a2-902f-6727ebaad0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, you can use the NaiveBayesClassifier class to build the model. Use the .train() method to train the model and the .accuracy() method to test the model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b39a000-398e-4df2-8b85-316772940b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9966666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2070.8 : 1.0\n",
      "                      :) = True           Positi : Negati =    992.5 : 1.0\n",
      "                follower = True           Positi : Negati =     36.8 : 1.0\n",
      "                     sad = True           Negati : Positi =     25.7 : 1.0\n",
      "                     bam = True           Positi : Negati =     18.9 : 1.0\n",
      "                    glad = True           Positi : Negati =     18.9 : 1.0\n",
      "                  arrive = True           Positi : Negati =     17.7 : 1.0\n",
      "                   enjoy = True           Positi : Negati =     16.6 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.4 : 1.0\n",
      "               community = True           Positi : Negati =     15.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d672a200-8908-4bff-8df5-2425cdf6617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f3e22-5aba-4942-8369-7cadfa1d52d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
