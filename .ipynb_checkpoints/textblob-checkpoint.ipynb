{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c5aa7c-8f15-4411-b0bd-93b6a78c6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk import word_tokenize \n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import speech_recognition as sr \n",
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca74ff1c-433d-4034-bd04-311b7ae2c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip = mp.VideoFileClip(r\"recording2.webm\") \n",
    "# clip.audio.write_audiofile(r\"converted.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a82795-3338-49ac-b938-689fcf21ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = sr.Recognizer()\n",
    "# r.pause_threshold = 20000\n",
    "# r.energy_threshold = 4000\n",
    "# audio = sr.AudioFile(\"converted.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d3a643-f4a0-4ffc-96b9-8c1476e94031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with audio as source:\n",
    "#     r.adjust_for_ambient_noise(source)\n",
    "#     audio_file = r.record(source,duration=10000)\n",
    "#     print(audio_file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad61367-e1a9-4ad3-bbed-887fdc4fd1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fd57d4-28b8-4477-88e2-2ac2a8ca1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('recognized.txt',mode ='w') as file: \n",
    "#     file.write(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cf53c9-a07d-476f-8e63-b12d2efe8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the function recognize_google() does is to call the google Speech API and get back the result. When I used the provided audio file, I got back the transcription of the first 30 seconds. That's due to the limitation of the free version of the Google speech API and has nothing to do with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695c4cb6-26df-40f0-acc8-70c6f0fe9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['n-gram', 'sentiment_score', 'sentiment_label'])\n",
    "df_results_vader = pd.DataFrame(columns=['n-gram', 'pos', 'neg', 'neu', 'compound', 'sentiment_label'])\n",
    "df_comparison = pd.DataFrame(columns=['n-gram', 'textblob-score', 'textblob-subjectivity', 'vader-pos', 'vader-neg', 'vader-neu', 'vader-compound'])\n",
    "df_azure_speech = pd.DataFrame(columns=['Sentence', 'Pre-processed sentence', 'textblob-subjectivity', 'textblob-score', 'vader-pos','textblob-polarity', 'vader-neg', 'vader-neu', 'vader-compound', 'vader-polarity'])\n",
    "pos_words_str = \"\"\n",
    "neg_words_str = \"\"\n",
    "pos_words_str_vader = \"\"\n",
    "neg_words_str_vader = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefbb3fe-7996-419d-9b13-2309e7301d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n-gram</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [n-gram, sentiment_score, sentiment_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0145a87-14ca-4a8e-b352-57de89a96f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accef47c-1d91-4419-acf3-49f46f2befda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript2.txt\") as transcript:\n",
    "    lines = transcript.readlines()\n",
    "#print(lines)\n",
    "# subor obsahuje len jeden riadok, inak by nam vratil list a kazdy riadok by bol prvok v liste\n",
    "#print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a866e950-a5a4-4bf7-8f2f-e82bbf3eaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_object = TextBlob(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f38d6f2a-45ec-4b0f-bc51-b4db6fe4a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ngram_object.ngrams(n=4) \n",
    "#print(ngrams) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f557db0-aebd-4ab5-ae91-de76991cc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ngram in ngrams:\n",
    "#     ngram_str = TextBlob(' '.join(ngram))\n",
    "#     sentiment = ngram_str.sentiment.polarity\n",
    "#     subjectivity = ngram_str.sentiment.subjectivity\n",
    "#     if sentiment >= 0.2:\n",
    "#         print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment) + ' | subjectivity: ' + str(subjectivity))\n",
    "#     if sentiment < 0:\n",
    "#         print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment) + ' | subjectivity: ' + str(subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8d63ed-dcc1-4519-af10-50a0c3361c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punct, lower case everything etc...\n",
    "# then ngrams -> how large?\n",
    "# then standard text pre processing for ngrams\n",
    "# then TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91bbc491-58e3-44a5-a83b-3c774bd9aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"e-lekar.txt\", 'r') as transcript:\n",
    "#     lines = transcript.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "263603f4-dc9e-436b-8ae8-9e1e705658e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"e-lekar.txt\", 'r') as transcript_sentences:\n",
    "    text = transcript_sentences.read()\n",
    "\n",
    "sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb248a6d-b215-4f8f-856c-cbcb8e8bc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in sentences:\n",
    "#     print(sentence)\n",
    "#     df_azure_speech.append({'Sentence': sentence}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eba522a7-3e05-42cd-8013-dcbb3654c5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Pre-processed sentence</th>\n",
       "      <th>textblob-subjectivity</th>\n",
       "      <th>textblob-score</th>\n",
       "      <th>vader-pos</th>\n",
       "      <th>textblob-polarity</th>\n",
       "      <th>vader-neg</th>\n",
       "      <th>vader-neu</th>\n",
       "      <th>vader-compound</th>\n",
       "      <th>vader-polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence, Pre-processed sentence, textblob-subjectivity, textblob-score, vader-pos, textblob-polarity, vader-neg, vader-neu, vader-compound, vader-polarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_azure_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4edb77e-6399-4f21-8c86-c78612571561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "lines_clean = []\n",
    "for sentence in sentences:\n",
    "    lines_clean.append(clean_sentences(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efbd34ca-34b8-4c89-adb7-67bf65135516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in lines_clean:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98444eba-41d4-49f1-bfb7-db9fdbafcc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16e61fce-1c23-425a-ad87-dc96ac63dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines_clean = clean(lines)\n",
    "# print(lines_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c727598f-b40d-4e44-9621-4a1fef924f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((stopwords.words('english')))\n",
    "stopwords_custom = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "766dcb72-5c5c-4ced-b554-7032f5e44153",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = {'no', 'not', 'nor'}\n",
    "item_list = [e for e in stopwords_custom if e not in unwanted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e415980b-8b7e-4a65-923e-2b0a3092589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(str(text)))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(item_list):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef32cdf8-f0f0-41dc-8d9d-fe2a8cd84e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_token_stop_pos = []\n",
    "for sentence in lines_clean:\n",
    "    lines_token_stop_pos.append(token_stop_pos(sentence))\n",
    "# print(lines_token_stop_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb67805c-5f46-4d35-b981-55a3672cc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a581f983-91b5-41ab-b7c4-b76fcd1b016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_lemma = []\n",
    "for phrase in lines_token_stop_pos:\n",
    "    lines_lemma.append(lemmatize(phrase).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db079a78-e088-4c60-a184-6a53f04c3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_object = TextBlob(lines_lemma)\n",
    "# ngrams = ngram_object.ngrams(n=5) \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#print(type(lines_lemma))\n",
    "\n",
    "# for li in lines_lemma:\n",
    "#     print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb04ad98-06fd-482e-babc-0f1ecf8bf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textblob_objects = TextBlob(lines_lemma)\n",
    "for sentence, line in zip(sentences, lines_lemma):\n",
    "    txtblob_object = TextBlob(line)\n",
    "    sentiment = txtblob_object.sentiment.polarity\n",
    "    subjectivity = txtblob_object.sentiment.subjectivity\n",
    "    sentiment_vader = analyzer.polarity_scores(txtblob_object)\n",
    "    # df_azure_speech = pd.DataFrame(columns=['Sentence', 'Pre-processed sentence', 'textblob-subjectivity', 'textblob-score', 'vader-pos','textblob-polarity', 'vader-neg', 'vader-neu', 'vader-compound', 'vader-polarity'])\n",
    "    df_azure_speech = df_azure_speech.append({'Sentence': sentence,'Pre-processed sentence': line, 'textblob-subjectivity': subjectivity, 'textblob-score': sentiment, 'vader-pos': sentiment_vader['pos'], 'vader-neg': sentiment_vader['neg'], 'vader-neu':sentiment_vader['neu'], 'vader-compound':sentiment_vader['compound']}, ignore_index=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72633e45-58f5-490a-9e9d-f6fcc4060223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_azure_speech.to_csv(\"results_elekar-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd6fb97f-b016-4907-a8ea-6bce2dd1c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngrams:\n",
    "    ngram_str = TextBlob(' '.join(ngram))\n",
    "    sentiment = ngram_str.sentiment.polarity\n",
    "    subjectivity = ngram_str.sentiment.subjectivity\n",
    "    sentiment_vader = analyzer.polarity_scores(ngram_str)\n",
    "    # (columns=['n-gram', 'textblob-score', 'textblob-subjectivity', 'vader-pos', 'vader-neg', 'vader-neu'])\n",
    "    # df_azure_speech = pd.DataFrame(columns=['Sentence', 'Pre-processed sentence', 'textblob-subjectivity', 'textblob-score', 'vader-pos','textblob-polarity', 'vader-neg', 'vader-neu', 'vader-compound', 'vader-polarity'])\n",
    "\n",
    "    df_comparison = df_comparison.append({'n-gram': ' '.join(ngram), 'textblob-score': sentiment, 'textblob-subjectivity': subjectivity, 'vader-pos': sentiment_vader['pos'], 'vader-neg': sentiment_vader['neg'], 'vader-neu':sentiment_vader['neu'], 'vader-compound':sentiment_vader['compound']}, ignore_index=True)\n",
    "    if subjectivity <= 0:\n",
    "        pass\n",
    "        # print(' '.join(ngram) + ' ' + str(ngram_str.sentiment.subjectivity))\n",
    "    if sentiment > 0:\n",
    "        df_results = df_results.append({'n-gram': ' '.join(ngram), 'sentiment_score': sentiment, 'sentiment_label': 'Positive'}, ignore_index=True)\n",
    "        pos_words_str += \" \" +  ' '.join(ngram)\n",
    "        # print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment) + ' | subjectivity: ' + str(subjectivity))\n",
    "    if sentiment < 0:\n",
    "        # print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment) + ' | subjectivity: ' + str(subjectivity))\n",
    "        df_results = df_results.append({'n-gram': ' '.join(ngram), 'sentiment_score': sentiment, 'sentiment_label': 'Negative'}, ignore_index=True)\n",
    "        neg_words_str += \" \" + ' '.join(ngram)\n",
    "    if sentiment == 0:\n",
    "        df_results = df_results.append({'n-gram': ' '.join(ngram), 'sentiment_score': sentiment, 'sentiment_label': 'Neutral'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b139de9-b704-468c-866d-4160389c636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison.to_csv(\"results_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e29c8c5-fc8e-4e5f-935b-84604ea297f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ngram in ngrams:\n",
    "#     ngram_str = TextBlob(' '.join(ngram), analyzer=NaiveBayesAnalyzer())\n",
    "#     sentiment = ngram_str.sentiment\n",
    "#     print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment))\n",
    "# # pomale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "525fec9a-65a0-4dfc-9f0c-5abe5fa60d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):\n",
    "    h = int(360.0 * 45.0 / 255.0)\n",
    "    s = int(100.0 * 255.0 / 255.0)\n",
    "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
    "\n",
    "    return \"hsl({}, {}%, {}%)\".format(h, s, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc98134e-511c-4185-a003-921250b8da93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20536/3123197903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"part_1.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n\u001b[0m\u001b[0;32m      4\u001b[0m                             \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                             \u001b[0mbackground_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\iau-1\\iau-1-venv\\env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \"\"\"\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\iau-1\\iau-1-venv\\env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m--> 613\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\documents\\iau-1\\iau-1-venv\\env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m         \u001b[1;31m# remove 's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "file_content=open (\"part_1.txt\").read()\n",
    "\n",
    "wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "                            stopwords = STOPWORDS,\n",
    "                            background_color = 'white',\n",
    "                            width = 1200,\n",
    "                            height = 1000,\n",
    "                            color_func = random_color_func\n",
    "                            ).generate(lines_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511b2d5-53cd-42d1-94f8-4cd3ba0bbaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# rozdelit na pos a neg word cloud\n",
    "# histogram sentimentu \n",
    "# vseobecny pocet sentimentu (pos, neg)\n",
    "# top neg, top pos\n",
    "# convert results to dataframe / csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f2ce3-75be-45d2-9fac-a8b1e96f11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "                            stopwords = STOPWORDS,\n",
    "                            background_color = 'white',\n",
    "                            width = 1200,\n",
    "                            height = 1000,\n",
    "                            color_func = random_color_func\n",
    "                            ).generate(pos_words_str)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475ee91-1434-4e03-8102-66bf862ad5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "                            stopwords = STOPWORDS,\n",
    "                            background_color = 'white',\n",
    "                            width = 1200,\n",
    "                            height = 1000,\n",
    "                            color_func = random_color_func\n",
    "                            ).generate(neg_words_str)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15841287-9cad-4b1a-b9aa-dc4e627374ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(textfile):\n",
    "    with open(textfile) as transcript:\n",
    "        lines = transcript.readlines()\n",
    "    lines_clean = clean(lines[0])\n",
    "    lines_token_stop_pos = token_stop_pos(lines_clean)\n",
    "    lines_lemma = lemmatize(lines_token_stop_pos).lower()\n",
    "    ngram_object = TextBlob(lines_lemma)\n",
    "    ngrams = ngram_object.ngrams(n=3) \n",
    "    for ngram in ngrams:\n",
    "        ngram_str = TextBlob(' '.join(ngram))\n",
    "        sentiment = ngram_str.sentiment.polarity\n",
    "        subjectivity = ngram_str.sentiment.subjectivity\n",
    "        # sentiment_vader = analyzer.polarity_scores(ngram_str)\n",
    "        if sentiment >= 0.2:\n",
    "            print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment) + ' | subjectivity: ' + str(subjectivity))\n",
    "        if sentiment < 0:\n",
    "            print('Ngram: ' + ' '.join(ngram) + ' | sentiment: ' + str(sentiment) + ' | subjectivity: ' + str(subjectivity))\n",
    "    \n",
    "    file_content=open (textfile).read()\n",
    "\n",
    "    wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "                            stopwords = STOPWORDS,\n",
    "                            background_color = 'white',\n",
    "                            width = 1200,\n",
    "                            height = 1000,\n",
    "                            color_func = random_color_func\n",
    "                            ).generate(lines_lemma)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b38e45-72bc-4d90-9b18-d29125ed1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"testing_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979866a2-81aa-427f-a9bd-c8ef95933925",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = TextBlob(\"best\")\n",
    "sentiment = str_.sentiment.polarity\n",
    "print(sentiment)\n",
    "\n",
    "str_ = TextBlob(\"not the best\")\n",
    "sentiment = str_.sentiment.polarity\n",
    "print(sentiment)\n",
    "\n",
    "str_ = TextBlob(\"not best\")\n",
    "sentiment = str_.sentiment.polarity\n",
    "print(sentiment)\n",
    "\n",
    "str_ = TextBlob(\"wasn't best\")\n",
    "sentiment = str_.sentiment.polarity\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbdf18-ebc5-4ec0-8912-6676a21a4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "a = \"it was very problematic\"\n",
    "b = TextBlob(a, analyzer=NaiveBayesAnalyzer())\n",
    "print(b.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60505278-9cb0-4a3d-a3b3-e03f779fbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The textblob.sentiments module contains two sentiment analysis implementations, PatternAnalyzer (based on the pattern library) and NaiveBayesAnalyzer (an NLTK classifier trained on a movie reviews corpus).\n",
    "# The default implementation is PatternAnalyzer, but you can override the analyzer by passing another implementation into a TextBlobâ€™s constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac8962-83da-4a3e-8456-76f5e0652c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = TextBlob(\"big problem\")\n",
    "sentiment = str_.sentiment.polarity\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23287f83-6a24-44ad-a566-6bd4102f837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a519e5-ccfc-444c-8f51-076c8952e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000d957-2c0f-4e82-b496-ef398ad9f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_results, x=\"sentiment_label\", y=\"sentiment_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e6909-8efe-4155-a007-2c1057ee4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sentiment_label.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b40ea-5b38-4976-ac3a-712e24d427a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values('sentiment_score').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b348d-d5f7-47be-953a-0a50067eec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values('sentiment_score', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91271c8b-c566-42ef-b9ea-d6ec8f16a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.array(Image.open('flower.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f9233-a719-4917-a68c-3aaf8ef41c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wc = WordCloud(stopwords=STOPWORDS, font_path=r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "#                mask=mask, background_color=\"white\",\n",
    "#                max_words=2000, max_font_size=256,\n",
    "#                random_state=42, width=mask.shape[1],\n",
    "#                height=mask.shape[0])\n",
    "# wc.generate(lines_lemma)\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# wc.to_file(\"wordcloud_flower.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c2a18-e3d8-4b60-a2f4-c0b4f14df916",
   "metadata": {},
   "source": [
    "### VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680ed83-8598-44f9-bc54-9f3c430e7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40325980/how-is-the-vader-compound-polarity-score-calculated-in-python-nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118d0d6-c80a-444f-8176-0c9da08e1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d7952-d1f9-4804-97ad-e5b638a9777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060400f8-c6c7-46ac-93c9-10cf871eee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngrams:\n",
    "    ngram_str = ' '.join(ngram)\n",
    "    sentiment = analyzer.polarity_scores(ngram_str)\n",
    "    if sentiment['compound'] < 0:\n",
    "        neg_words_str_vader += \" \" +  ' '.join(ngram)\n",
    "        df_results_vader = df_results_vader.append({'n-gram': ' '.join(ngram), 'pos': sentiment['pos'], 'neg': sentiment['neg'], 'neu':sentiment['neu'], 'compound': sentiment['compound'], 'sentiment_label': 'Negative'}, ignore_index=True)\n",
    "    if sentiment['compound'] > 0:\n",
    "        pos_words_str_vader += \" \" +  ' '.join(ngram)\n",
    "        df_results_vader = df_results_vader.append({'n-gram': ' '.join(ngram), 'pos': sentiment['pos'], 'neg': sentiment['neg'], 'neu':sentiment['neu'], 'compound': sentiment['compound'], 'sentiment_label': 'Positive'}, ignore_index=True)\n",
    "    if sentiment['compound'] == 0:\n",
    "        df_results_vader = df_results_vader.append({'n-gram': ' '.join(ngram), 'pos': sentiment['pos'], 'neg': sentiment['neg'], 'neu':sentiment['neu'], 'compound': sentiment['compound'], 'sentiment_label': 'Neutral'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b8ce9-ccda-4bf8-9bed-f0a2cf0ad427",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "                            stopwords = STOPWORDS,\n",
    "                            background_color = 'white',\n",
    "                            width = 1200,\n",
    "                            height = 1000,\n",
    "                            color_func = random_color_func\n",
    "                            ).generate(neg_words_str_vader)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74823c2f-a243-4ea5-b293-e22a6525dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path = r'C:\\Windows\\Fonts\\Verdana.ttf',\n",
    "                            stopwords = STOPWORDS,\n",
    "                            background_color = 'white',\n",
    "                            width = 1200,\n",
    "                            height = 1000,\n",
    "                            color_func = random_color_func\n",
    "                            ).generate(pos_words_str_vader)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37935dd7-8def-492c-bfa0-ce5fe50e3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cafd7-8f3b-488c-819e-dfbaa7eede4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_vader.sort_values('compound', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77068117-b45d-4bd6-abe4-cf2fbdd237e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_vader.sort_values('compound').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0adeb-5950-4e77-894a-c2ae170bdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_vader.sentiment_label.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fffda4-e2c5-4283-922b-a38043e33062",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_results_vader.compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6267182-db39-4af7-b7c3-8815b4bbab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_results_vader, x=\"sentiment_label\", y=\"compound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271473a-69de-43dc-99da-29a675917e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
